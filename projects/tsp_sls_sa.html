<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Wenting's Project - Stochastic Local Search with Simulated Annealing</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

		<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
		<script>
			$(function(){
				$("#contact").load("../contact.html");
			});
		</script>
	</head>
	<body class="is-preload">
		<div class="back-link">
			<a href="javascript:history.go(-1)" style="border-bottom: none; ">
				<span class="icon solid fa-arrow-left" style="font-size: 24px"></span>
			</a>
		</div>
		<!-- Main -->
			<div id="project">
				<section id="one">
					<header class="major">
						<h2>Stochastic Local Search with Simulated Annealing to TSP</h2>
					</header>
					<p>
						Stochastic Local Search (SLS) is a general optimization technique used to find approximate solutions to combinatorial optimization problems. Within the realm of SLS, "simple SLS methods" permit exploration beyond local optima by considering worsening neighbors. In contrast to complex strategies, these methods rely on a single search step within a fixed neighborhood relation (Hoos & Stützle, 2015, section 54.3). Simulated Annealing (SA) is a prime example, employing a temperature-based strategy to accept moves to worse solutions. The acceptance of worsening solutions is governed by a temperature parameter, which is gradually reduced over the course of the optimization process according to a predefined schedule.


						<a href="#demo" style="border-bottom: none; "> See Pseudocode
							<span class="icon solid fa-arrow-right"></span>
						</a>
					</p>
				</section>
				<section>
					<header class="major">
						<h2>Methodology</h2>
					</header>
					<p>
						Neighbors function: The algorithm starts by generating a random initial solution, denoted as s, for the TSP problem. This function generates all possible neighbors of a solution by swapping two cities of the given solution to a different position.


					</p>
					<p>
						SLS_with_SA function: The algorithm starts by generating a random initial solution, denoted as s, for the TSP problem. The main loop iterates from 1 to a specified maximum number of iterations (t). The temperature T is determined by the cooling schedule function schedule(t).

					</p>
					<ul>
						<li>Check for Termination: If the temperature T drops to zero, the algorithm terminates, and the current solution s is returned.</li>
						<li>Neighbor Generation: A neighboring solution s_next is randomly chosen from the set of neighbors of the current solution s. We generate the neighbor solution by swapping two cities in the path.</li>
						<li>Evaluate the Energy (Line 6): The energy E is computed as the difference in cost between the new solution s_next and the current solution s.</li>
						<li>Acceptance Criteria (Lines 7-9): If s_next is better (lower cost) than the current solution s, it is accepted. If s_next is worse, its acceptance probability is determined by the e<sup>-&Delta;E/T</sup>. The acceptance rate drops as the temperature T decreases, allowing the algorithm to explore more at the beginning but converge towards the end.</li>
					</ul>
				</section>



				<section id="demo">
					<h2>Pseudocode</h2>
<pre>
function Neighbors(solution):
    neighbors = {}
    for city in solution.nodes:
        for other_city in solution.nodes:
            s_new = copy(solution)
            s_new.swap(city, other_city)
            neighbors.add(s_new)
    return neighbors

function SLS_with_SA(problem, schedule) returns solution state s :
    s = Random_Solution(problem)
    for t = 1 to infinity do
        T = schedule(t)
        if T = 0 :
            return s
        s_next = randomly pick one from Neighbors(s)
        E = value(s_next) – value(s)
        if E < 0  ||  rand() < e-E/T :
            s = s_next
</pre>


				</section>

				<section>
					<header class="major">
						<h2>Result</h2>
					</header>
					<p>
						A neighboring solution can be generated by swapping positions of two cities in the path. Therefore, the cost of this neighbor solution can be efficiently computed based on the cost of the current solution, eliminating the need to iterate through all N cities. This allows for a constant-time computation of the cost, resulting in a <i>O(1)</i> time complexity in each iteration of SA. For the overall SA algorithm, given N cities, we need <i>O(N)</i> time and <i>O(N)</i> space to compute the initial solution. If M is the number of iterations of the for loop, the overall complexity becomes <i>O(N+M)</i>, with space complexity limited to <i>O(N)</i> as only one current solution and one neighbor solution need to be tracked.
					</p>
					<div style="display: inline;">
						<img width="100%" src="../images/fulls/sls_sa_fig1.png" alt="Figure1" />
					</div>
					<p>
						The SLS (Simulated Annealing) algorithm performs well for the 10-city instance, surpassing the BnB DFS. The success in small instances suggests that SLS can effectively navigate simple solution spaces and converge to high-quality solutions within a reasonable runtime. However, for larger instances (25, 50, 100, 150, and 200 cities), the minimum cost obtained from the SLS algorithm is not as competitive as the BnB DFS algorithm. This suggests that while SLS excels in navigating simpler solution spaces, it encounters difficulties exploring the intricate solution space systematically in larger problems, potentially getting stuck in local minima. Due to this nature, SLS may serve as a more valuable tool to quickly find suboptimal solutions rather than optimal solutions.


					</p>
					<p>
						Besides performance, the runtime observation aligns with our analysis of the time complexity of SLS, expressed as O(N+M), where N is the number of cities and M is the number of iterations. This correlation is reflected in our results, indicating that as the number of cities increases, both the runtime and the complexity of the problem escalate. Furthermore, the runtime is notably contingent on the number of iterations. This high dependence on iterations is anticipated, particularly when N is relatively smaller than M. In this way, the runtime of the SLS outperforms the BnB DFS greatly.

					</p>

					<p>
						SLS’s number of iterations is influenced by parameters such as initial temperature, alpha (cooling rate), and max iteration. In our performance evaluation, we maintain max_iteration > actual_iteration and set initial_temperature to 10 for simplicity. By changing the value of alpha, we can adjust the number of iterations as needed. However, the performance of the SLS algorithm can be further enhanced through careful parameter tuning. This aspect could be explored in future work.
					</p>

				</section>

				<section id="four">
					<div id="contact"></div>
				</section>


			</div>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/jquery.poptrox.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	</body>
</html>